{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models on FS-Mol\n",
    "\n",
    "`fs_mol.utils.test_utils.eval_model()` is a utility function that allows to evaluate a model against the full set of FS-Mol testing tasks, following a fixed evaluation scheme.\n",
    "\n",
    "It requires the following arguments:\n",
    "* `test_model_fn: Callable[[FSMolTaskSample, str, int], BinaryEvalMetrics]`:\n",
    "  This is the core evaluation function, taking a `FSMolTaskSample` (that is, a sample of datapoints from a single task -- see notebooks/dataset.ipynb), a temporary output folder in which to store scratch data, and the seed used for the sampling.\n",
    "  It should return `BinaryEvalMetric` object containing all metrics calculated from the model output and labels of the task sample.\n",
    "* `dataset: FSMolDataset`:\n",
    "  The actual dataset, which can be the full FS-Mol dataset as released, or a different set created directly through its constructor.\n",
    "  This is particularly useful if you are planning to evaluate models on a set of tasks differing from FS-Mol's test set.\n",
    "\n",
    "The returned results contain a list of evaluation metrics for each task, in a dictionary indexed by task name.\n",
    "\n",
    "Additionally, it can be optionally further configured using the following optional arguments:\n",
    "* `out_dir: Optional[str]`:\n",
    "   If provided, a summary of the evaluation results will be written, as one CSV file per task.\n",
    "* `seed: int`:\n",
    "   A seed value used to make sampling and splitting of tasks deterministic.\n",
    "   If set to anything but `0`, results of `eval_model` will be incomparable to the canonical evaluation runs.\n",
    "* `num_samples: int`:\n",
    "   The number of random splits to draw for the task undergoing evaluation.\n",
    "* `train_set_sample_sizes: List[int]`:\n",
    "   The sizes of training / support set data to consider. For each of these, `num_samples` samples will be drawn for each test task, and then be used to call `test_model_fn` with, where the passed `FSMolTaskSample` will have the requested number of `train_samples`.\n",
    "* `valid_size_or_ratio: Union[int, float]`:\n",
    "   Number or ratio of `train_samples` in the drawn samples that will be split out into `validation_samples`. This is useful for models that require an explicit validation set during fine-tuning.\n",
    "* `test_size_or_ratio: Optional[Union[int, float, Tuple[int, int]]]`:\n",
    "   Number or ratio of `test_samples` that will be drawn from the original task. By default, all available samples will be used, but it may be useful to limit this when not using `eval_model` for full model evaluation.\n",
    "* `fold: DataFold`:\n",
    "   The fold of FS-Mol on which to perform evaluation, typically will be the test fold.\n",
    "* `task_reader_fn: Optional[Callable[[List[RichPath], int], Iterable[FSMolTask]]]`:\n",
    "   Callable allowing additional transformations on the data prior to its batching and passing through a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Evaluating a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"FS-Mol\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'],\"FS-Mol\",\"datasets\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a simple implementation of a `test_model_fn` that creates a random forest model, using the scikit-learn default implementation and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from fs_mol.data.fsmol_task import FSMolTaskSample\n",
    "from fs_mol.utils.metrics import BinaryEvalMetrics, compute_binary_task_metrics\n",
    "\n",
    "def test_model_fn(\n",
    "    task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    ") -> BinaryEvalMetrics:\n",
    "    train_data = task_sample.train_samples\n",
    "    test_data = task_sample.test_samples\n",
    "\n",
    "    # get data in to form for sklearn\n",
    "    X_train = np.array([x.get_fingerprint() for x in train_data])\n",
    "    X_test = np.array([x.get_fingerprint() for x in test_data])\n",
    "    y_train = [float(x.bool_label) for x in train_data]\n",
    "    y_test = [float(x.bool_label) for x in test_data]\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute test results:\n",
    "    y_predicted_true_probs = model.predict_proba(X_test)[:, 1]\n",
    "    test_metrics = compute_binary_task_metrics(y_predicted_true_probs, y_test)\n",
    "\n",
    "    return test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these functions, we can now evaluate these models on the test tasks in FS-Mol as follows, reducing the space of considered samples from each task for speed purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fs_mol.data.fsmol_dataset import FSMolDataset, DataFold\n",
    "from fs_mol.utils.test_utils import eval_model\n",
    "\n",
    "fsmol_dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "\n",
    "results = eval_model(\n",
    "    test_model_fn=test_model_fn,\n",
    "    dataset=fsmol_dataset,\n",
    "    # Restrict number of samples to one per task:\n",
    "    train_set_sample_sizes=[16],\n",
    "    num_samples=1,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91c2cdb63b030871da94aa046e171a8c212268d3e9d71e3496f8c89eaedb0da0"
  },
  "kernelspec": {
   "display_name": "Python [conda env:fsmol] *",
   "language": "python",
   "name": "conda-env-fsmol-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847b9d70",
   "metadata": {},
   "source": [
    "# Using the FSMol Dataset\n",
    "\n",
    "The `FSMolDataset` class provides access to the train/valid/test tasks of our few-shot learning dataset on molecules.\n",
    "\n",
    "An instance is created from the data directory by `FSMolDataset.from_directory(/path/to/dataset)`.\n",
    "The data in the `train`, `validation` and `test` folds of the dataset can be accessed using `FSMolDataset.get_task_reading_iterable`.\n",
    "By default, this method simply reads in the individual data files using a number of background worker processes and provides them in a standard format, but this behavior can be customized by providing a specialized callback function.\n",
    "The default implementation returns an iterable over `FSMolTask` objects, each containing an entire task's of single featurised molecules, `MoleculeDatapoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1268aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "#FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\")\n",
    "FS_MOL_CHECKOUT_PATH = (\"/home/amna/FS-Mol\")\n",
    "#FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\", \"datasets\")\n",
    "FS_MOL_DATASET_PATH = (\"/home/amna/FS-Mol/datasets\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774baa4d",
   "metadata": {},
   "source": [
    "## Example: Creating FSMolDataset and Accessing a Task\n",
    "\n",
    "First, we simply create a dataset and have a look at a single task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69131d38",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16616/220421258.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_task_iterable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task_reading_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALIDATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_task_iterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FS-Mol/fs_mol/data/file_reader_iterable.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Reached if we break out of the while loop because we ran out of data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fs_mol.data import FSMolDataset, DataFold\n",
    "dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "\n",
    "valid_task_iterable = dataset.get_task_reading_iterable(DataFold.VALIDATION)\n",
    "task = next(iter(valid_task_iterable))\n",
    "print(task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f429bfa",
   "metadata": {},
   "source": [
    "A task, by default, is simply a name and a list of datapoints, each stored as a `MoleculeDatapoint`, a dataclass with the following definition:\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class MoleculeDatapoint:\n",
    "    \"\"\"Data structure holding information for a single molecule.\n",
    "\n",
    "    Args:\n",
    "        task_name: String describing the task this datapoint is taken from.\n",
    "        smiles: SMILES string describing the molecule this datapoint corresponds to.\n",
    "        graph: GraphData object containing information about the molecule in graph representation\n",
    "            form, according to featurization chosen in preprocessing.\n",
    "        numeric_label: numerical label (e.g., activity), usually measured in the lab\n",
    "        bool_label: bool classification label, usually derived from the numeric label using a\n",
    "            threshold.\n",
    "        fingerprint: optional ECFP for the molecule.\n",
    "        descriptors: optional phys-chem descriptors for the molecule.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str\n",
    "    smiles: str\n",
    "    graph: GraphData\n",
    "    numeric_label: float\n",
    "    bool_label: bool\n",
    "    fingerprint: Optional[np.ndarray]\n",
    "    descriptors: Optional[np.ndarray]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502d741d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'task' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16616/1181071624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'task' is not defined"
     ]
    }
   ],
   "source": [
    "task.samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7f3675",
   "metadata": {},
   "source": [
    "## Example: Making a Task Sample\n",
    "\n",
    "In practice, all methods require that we sample from the `FSMolTask`s, for example during evaluation.\n",
    "To this end, we have implemented a number of samplers, extensions of the `TaskSampler` abstract class, which provides a unified `sample` method.\n",
    "The baseline models implemented in this repository use stratified sampling (implemented in `StratifiedTaskSampler`), but `RandomTaskSampler` and `BalancedTaskSampler` exist as alternatives.\n",
    "\n",
    "In practice, sampling often requires additional parameters, such as a minimal support set size, a minimal query set size, or an upper desired query set size. When a task dataset cannot be sampled using these parameters (e.g., because it's too small), a `SamplingException` exception is thrown and the caller can decide how to handle this. For example, during training, it is often reasonable to simply ignore such exceptions and pass over failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "658e1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data import StratifiedTaskSampler\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio = 16,\n",
    "    valid_size_or_ratio = 0.0,\n",
    "    test_size_or_ratio = 256, \n",
    "    allow_smaller_test = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c164adf",
   "metadata": {},
   "source": [
    "Applying a sampler returns a sample from the task, which contains a support/train set, validation set, and query/test set. In this case, the task is too small to return all requested testing samples, so it returns the maximum available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c107714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in task: 358\n",
      "Number of train samples: 16\n",
      "Number of test samples: 256\n",
      "Number of valid samples: 0\n"
     ]
    }
   ],
   "source": [
    "task_sample = task_sampler.sample(task, seed=0)\n",
    "\n",
    "print(f\"Number of samples in task: {len(task.samples)}\")\n",
    "print(f\"Number of train samples: {len(task_sample.train_samples)}\")\n",
    "print(f\"Number of test samples: {len(task_sample.test_samples)}\")\n",
    "print(f\"Number of valid samples: {len(task_sample.valid_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d505f",
   "metadata": {},
   "source": [
    "The samplers are built such that using the same seed will always return the same data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3347ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task sample, seed 0, first call  - First training SMILES O=C(CNC(=O)c1ccc(Cl)cc1)OC(C(=O)Nc1cc(C(F)(F)F)ccc1Cl)c1ccccc1\n",
      "Task sample, seed 0, second call - First training SMILES O=C(CNC(=O)c1ccc(Cl)cc1)OC(C(=O)Nc1cc(C(F)(F)F)ccc1Cl)c1ccccc1\n",
      "Task sample, seed 1, first call  - First training SMILES CNc1oc(C=Cc2cc(OC)c(OC)c(OC)c2)nc1C#N\n"
     ]
    }
   ],
   "source": [
    "task_sample_0 = task_sampler.sample(task, seed=0)\n",
    "task_sample_1 = task_sampler.sample(task, seed=1)\n",
    "\n",
    "print(f\"Task sample, seed 0, first call  - First training SMILES {task_sample.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 0, second call - First training SMILES {task_sample_0.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 1, first call  - First training SMILES {task_sample_1.train_samples[0].smiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984e47e",
   "metadata": {},
   "source": [
    "## Example: Custom Task Reading Functions in MAML\n",
    "\n",
    "When implementing models on top of FS-Mol, it is often useful to specialize the callback used in `get_task_reading_iterable`, for example to directly draw appropriate samples or do further featurization.\n",
    "\n",
    "In MAML, we for example need to use stratified samples in each training step:\n",
    "\n",
    "```python\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio=train_size,\n",
    "    valid_size_or_ratio=0,\n",
    "    test_size_or_ratio=(min_test_size, test_size),\n",
    ")\n",
    "\n",
    "def read_and_sample_from_task(paths: List[RichPath], id: int) -> Iterable[FSMolTaskSample]:\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        yield task_sampler.sample(task, seed=id + i)\n",
    "\n",
    "train_task_samples = dataset.get_task_reading_iterable(\n",
    "    data_fold=DataFold.TRAIN, task_reader_fn=read_and_sample_from_task\n",
    ")\n",
    "```\n",
    "\n",
    "In this instance, `train_task_samples` will now be an `Iterable` of sampled tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70bb17",
   "metadata": {},
   "source": [
    "## Batching Task Samples\n",
    "\n",
    "The `fs_mol.data` package also provides infrastructure for minibatching, using the `FSMolBatcher` class.\n",
    "These are framework-agnostic, and are used both for TensorFlow (in our MAML baseline) and Torch (in our MAT and Multitask baselines).\n",
    "Concretely, an `FSMolBatcher` object can be used to turn a list of datapoints into a sequence of minibatches.\n",
    "Our graph models handle batches of graphs as a single graph in which the samples appear as disconnected components. This is already implemented by our default implementation of `FSMolBatcher`, and so consumers only need to provide thing extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc35d1",
   "metadata": {},
   "source": [
    "### Example: GNN Multitask Batching\n",
    "\n",
    "As example, consider the GNN Multitask model (see `fs_mol/data/multitask.py` for full code), which needs to include the task ID for each sample. To do this, we extend the `FSMolBatcher` class using the callback hooks for initializing, extending and finalizing a batch:\n",
    "\n",
    "```python\n",
    "def multitask_batcher_init_fn(batch_data: Dict[str, Any]):\n",
    "    batch_data[\"sample_to_task_id\"] = []\n",
    "\n",
    "def multitask_batcher_add_sample_fn(\n",
    "    batch_data: Dict[str, Any],\n",
    "    sample_id: int,\n",
    "    sample: MoleculeDatapoint,\n",
    "    task_name_to_id: Dict[str, int],\n",
    "):\n",
    "    batch_data[\"sample_to_task_id\"].append(task_name_to_id[sample.task_name])\n",
    "\n",
    "def multitask_batcher_finalizer_fn(\n",
    "    batch_data: Dict[str, Any]\n",
    ") -> Tuple[FSMolMultitaskBatch, np.ndarray]:\n",
    "    plain_batch = fsmol_batch_finalizer(batch_data)\n",
    "    return (\n",
    "        FSMolMultitaskBatch(\n",
    "            sample_to_task_id=np.stack(batch_data[\"sample_to_task_id\"], axis=0),\n",
    "            **dataclasses.asdict(plain_batch),\n",
    "        ),\n",
    "        np.stack(batch_data[\"bool_labels\"], axis=0),\n",
    "    )\n",
    "\n",
    "def get_multitask_batcher(\n",
    "    task_name_to_id: Dict[str, int],\n",
    "    max_num_graphs: Optional[int] = None,\n",
    "    max_num_nodes: Optional[int] = None,\n",
    "    max_num_edges: Optional[int] = None,\n",
    ") -> FSMolBatcher[FSMolMultitaskBatch, np.ndarray]:\n",
    "    return FSMolBatcher(\n",
    "        max_num_graphs=max_num_graphs,\n",
    "        max_num_nodes=max_num_nodes,\n",
    "        max_num_edges=max_num_edges,\n",
    "        init_callback=multitask_batcher_init_fn,\n",
    "        per_datapoint_callback=partial(\n",
    "            multitask_batcher_add_sample_fn, task_name_to_id=task_name_to_id\n",
    "        ),\n",
    "        finalizer_callback=multitask_batcher_finalizer_fn,\n",
    "    )\n",
    "```\n",
    "\n",
    "This batcher is then used to create batches of up to `num_chunked_tasks` loaded in parallel as follows:\n",
    "\n",
    "```python\n",
    "def paths_to_mixed_samples(\n",
    "    paths: List[RichPath], idx: int\n",
    ") -> Iterable[Tuple[FSMolMultitaskBatch, np.ndarray]]:\n",
    "    loaded_samples: List[MoleculeDatapoint] = []\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        task_sample = self._task_sampler.sample(task, seed=idx + i)\n",
    "        loaded_samples.extend(task_sample.train_samples)\n",
    "    if self._data_fold == DataFold.TRAIN:\n",
    "        np.random.shuffle(loaded_samples)\n",
    "\n",
    "    for features, labels in self._batcher.batch(loaded_samples):\n",
    "        yield features, labels\n",
    "\n",
    "task_iterable = self._dataset.get_task_reading_iterable(\n",
    "    data_fold=self._data_fold,\n",
    "    task_reader_fn=paths_to_mixed_samples,\n",
    "    reader_chunk_size=self._num_chunked_tasks,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dee1af",
   "metadata": {},
   "source": [
    "### Example: MAML Batching\n",
    "\n",
    "We can similarly use this from a Tensorflow model, namely our MAML implementation (see `fs_mol/data/maml.py`). Here, we created a `TFGraphBatchIterable` class that uses a `FSMolBatcher` using a customized `finalizer_callback` to produce a batch suitable for consumption in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a4c9c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'node_features': array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 1., 0., ..., 1., 0., 0.],\n",
      "       [0., 1., 0., ..., 1., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 1., 0., ..., 1., 0., 1.],\n",
      "       [0., 0., 1., ..., 1., 0., 1.]], dtype=float32), 'node_to_graph_map': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32), 'num_graphs_in_batch': 3, 'adjacency_list_0': array([[ 1,  2],\n",
      "       [ 2,  3],\n",
      "       [ 3,  4],\n",
      "       [ 4,  6],\n",
      "       [ 7,  8],\n",
      "       [ 9, 10],\n",
      "       [ 9, 11],\n",
      "       [ 1, 13],\n",
      "       [13, 14],\n",
      "       [14, 15],\n",
      "       [15, 17],\n",
      "       [17, 18],\n",
      "       [19, 20],\n",
      "       [20, 21],\n",
      "       [21, 22],\n",
      "       [21, 23],\n",
      "       [21, 24],\n",
      "       [25, 26],\n",
      "       [27, 28],\n",
      "       [14, 29],\n",
      "       [30, 31],\n",
      "       [32, 33],\n",
      "       [12,  6],\n",
      "       [27, 18],\n",
      "       [34, 29],\n",
      "       [36, 37],\n",
      "       [38, 39],\n",
      "       [39, 40],\n",
      "       [40, 41],\n",
      "       [42, 43],\n",
      "       [44, 45],\n",
      "       [40, 47],\n",
      "       [48, 49],\n",
      "       [50, 51],\n",
      "       [53, 54],\n",
      "       [54, 55],\n",
      "       [55, 56],\n",
      "       [57, 58],\n",
      "       [60, 37],\n",
      "       [46, 41],\n",
      "       [52, 47],\n",
      "       [59, 55],\n",
      "       [62, 63],\n",
      "       [64, 65],\n",
      "       [65, 66],\n",
      "       [66, 67],\n",
      "       [67, 68],\n",
      "       [68, 69],\n",
      "       [68, 70],\n",
      "       [71, 72],\n",
      "       [72, 73],\n",
      "       [73, 74],\n",
      "       [75, 76],\n",
      "       [76, 77],\n",
      "       [78, 79],\n",
      "       [79, 80],\n",
      "       [82, 63],\n",
      "       [81, 74]], dtype=int32), 'edge_features_0': array([], shape=(58, 0), dtype=float32), 'adjacency_list_1': array([[ 0,  1],\n",
      "       [ 4,  5],\n",
      "       [ 6,  7],\n",
      "       [ 8,  9],\n",
      "       [11, 12],\n",
      "       [15, 16],\n",
      "       [18, 19],\n",
      "       [20, 25],\n",
      "       [26, 27],\n",
      "       [29, 30],\n",
      "       [31, 32],\n",
      "       [33, 34],\n",
      "       [37, 38],\n",
      "       [41, 42],\n",
      "       [43, 44],\n",
      "       [45, 46],\n",
      "       [47, 48],\n",
      "       [49, 50],\n",
      "       [51, 52],\n",
      "       [39, 53],\n",
      "       [56, 57],\n",
      "       [58, 59],\n",
      "       [54, 60],\n",
      "       [63, 64],\n",
      "       [65, 71],\n",
      "       [74, 75],\n",
      "       [76, 78],\n",
      "       [79, 81],\n",
      "       [72, 82]], dtype=int32), 'edge_features_1': array([], shape=(29, 0), dtype=float32), 'adjacency_list_2': array([[35, 36],\n",
      "       [61, 62]], dtype=int32), 'edge_features_2': array([], shape=(2, 0), dtype=float32)}, {'target_value': array([0., 1., 1.], dtype=float32)})\n"
     ]
    }
   ],
   "source": [
    "from fs_mol.data.maml import TFGraphBatchIterable\n",
    "batched_data = TFGraphBatchIterable(\n",
    "    samples=task_sample.train_samples,\n",
    "    shuffle=True,\n",
    "    max_num_nodes=100,\n",
    ")\n",
    "\n",
    "print(next(iter(batched_data)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e06fcc65451699fab52210cecc89ce74d347871d8379f3a65371b5502fcda228"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
